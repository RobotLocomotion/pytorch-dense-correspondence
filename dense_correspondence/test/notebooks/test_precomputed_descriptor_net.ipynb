{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "# key_dynam\n",
    "from key_dynam.utils.utils import get_project_root, load_yaml, get_data_root\n",
    "from key_dynam.dataset.drake_sim_episode_reader import DrakeSimEpisodeReader\n",
    "from key_dynam.utils import transform_utils\n",
    "from key_dynam.utils import drake_image_utils\n",
    "from key_dynam.dense_correspondence.descriptor_net import sample_descriptors, PrecomputedDescriptorNet\n",
    "\n",
    "# pdc\n",
    "from dense_correspondence.dataset.dynamic_drake_sim_dataset import DynamicDrakeSimDataset\n",
    "from dense_correspondence.correspondence_tools.correspondence_finder import reproject_pixels\n",
    "from dense_correspondence.correspondence_tools import correspondence_plotter\n",
    "from dense_correspondence.correspondence_tools.correspondence_finder import compute_correspondence_data, pad_correspondence_data\n",
    "from dense_correspondence_manipulation.utils.utils import getDenseCorrespondenceSourceDir, set_cuda_visible_devices\n",
    "import dense_correspondence.loss_functions.utils as loss_utils\n",
    "import dense_correspondence_manipulation.utils.utils as pdc_utils\n",
    "import dense_correspondence_manipulation.utils.visualization as vis_utils\n",
    "from dense_correspondence.network.predict import get_argmax_l2\n",
    "\n",
    "\n",
    "GPU_LIST = [1]\n",
    "set_cuda_visible_devices(GPU_LIST)\n",
    "\n",
    "# toggle to use the previous deprecated pdc image transform\n",
    "USE_DEPRECATED_IMAGE_TRANSFORM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network\n",
    "\n",
    "# model_name = \"2019-12-03-21-25-46-128603_top_down_same_view\"\n",
    "# model_name = \"2019-12-03-22-51-17-805212_top_down_rotated\"\n",
    "# model_name = \"2019-12-03-23-40-30-486661_top_down_rotated_sigma_5\"\n",
    "model_name = \"2019-12-04-01-32-12-010393_top_down_rotated_sigma_5\"\n",
    "network_folder = os.path.join(get_project_root(), \"data/dev/experiments/05/trained_models\", model_name)\n",
    "\n",
    "epoch = 155\n",
    "model_file = os.path.join(network_folder, \"net_dy_epoch_%d_iter_0_model.pth\" %(epoch))\n",
    "print(\"model_file\", model_file)\n",
    "model = torch.load(model_file)\n",
    "model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRECOMPUTED_DESCRIPTORS = True\n",
    "DATA_ROOT = get_data_root()\n",
    "# DATASET_NAME = \"2019-11-26-00-03-48-223155\"\n",
    "# DATASET_NAME = \"top_down_same_view\"\n",
    "DATASET_NAME = \"top_down_rotated\"\n",
    "# DATASET_NAME = \"2019-12-05-15-58-48-462834_top_down_rotated_250\"\n",
    "\n",
    "dataset_root = os.path.join(get_project_root(), \"data/dev/experiments/05/data\", DATASET_NAME)\n",
    "\n",
    "des_images_root = None\n",
    "if LOAD_PRECOMPUTED_DESCRIPTORS:\n",
    "    des_images_root = os.path.join(DATA_ROOT, \n",
    "                                   \"dev/experiments/05/precomputed_descriptors\", \n",
    "                                   DATASET_NAME, \n",
    "                                   model_name)\n",
    "    \n",
    "    metadata = load_yaml(os.path.join(des_images_root, 'metadata.yaml'))\n",
    "    \n",
    "    assert model_file == metadata['model_file']\n",
    "        \n",
    "    \n",
    "multi_episode_dict = DrakeSimEpisodeReader.load_dataset(dataset_root, \n",
    "                                                        descriptor_images_root=des_images_root)\n",
    "# make pdc dataset now\n",
    "\n",
    "\n",
    "# placeholder for now\n",
    "config_file = os.path.join(getDenseCorrespondenceSourceDir(), \n",
    "                           'config/dense_correspondence/global/drake_sim_dynamic.yaml')\n",
    "\n",
    "config = load_yaml(config_file)\n",
    "\n",
    "\n",
    "datasets = dict()\n",
    "for phase in [\"train\", \"valid\"]:\n",
    "    dataset = DynamicDrakeSimDataset(config, multi_episode_dict, phase=\"train\")\n",
    "    \n",
    "    if USE_DEPRECATED_IMAGE_TRANSFORM:\n",
    "        from dense_correspondence_manipulation.utils.torch_utils import get_deprecated_image_to_tensor_transform\n",
    "        dataset._rgb_image_to_tensor = get_deprecated_image_to_tensor_transform()\n",
    "        \n",
    "    datasets[phase] = dataset\n",
    "\n",
    "\n",
    "episode_name_list = list(multi_episode_dict.keys())\n",
    "episode_name_list.sort()\n",
    "episode_0 = multi_episode_dict[episode_name_list[0]]\n",
    "\n",
    "\n",
    "print(\"episode_0.name\", episode_0.name)\n",
    "\n",
    "camera_names = list(episode_0.camera_names)\n",
    "camera_name_a = camera_names[0]\n",
    "camera_name_b = camera_names[1]\n",
    "\n",
    "global_config = load_yaml(os.path.join(get_project_root(), 'experiments/05/config.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select reference descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "pdc_utils.reset_random_seed()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "print(\"device:\")\n",
    "idx = 0\n",
    "idx_step = 5\n",
    "\n",
    "MODEL_ENABLED = True\n",
    "DATA_TYPE = \"valid\"\n",
    "NUM_REF_DESCRIPTORS = 5\n",
    "\n",
    "sz = 2\n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "figsize = (6.4*sz*num_cols, 4.8*sz*num_rows)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # indices\n",
    "    b = 0\n",
    "    n = 10\n",
    "\n",
    "    data = None\n",
    "    dataset_tmp = None\n",
    "    dataset_tmp = datasets[DATA_TYPE]\n",
    "#     if DATA_TYPE == \"train\":\n",
    "#         dataset_tmp = dataset\n",
    "#     elif DATA_TYPE == \"valid\":\n",
    "#         dataset_tmp = dataset_valid\n",
    "        \n",
    "    \n",
    "    data = dataset_tmp._getitem(episode_0, \n",
    "                                idx,\n",
    "                                camera_name_a=camera_name_a, \n",
    "                                camera_name_b=camera_name_b)\n",
    "    \n",
    "    data_a = data['data_a']\n",
    "    rgb_tensor_a = data_a['rgb_tensor'].to(device).unsqueeze(0)\n",
    "#     rgb_tensor_b = data['data_b']['rgb_tensor'].to(device).unsqueeze(0)\n",
    "    rgb_a = data_a['rgb']\n",
    "#     uv_a = data['matches']['uv_a'].unsqueeze(0).to(device) # [B, 2, N]\n",
    "#     uv_b = data['matches']['uv_b'].unsqueeze(0).to(device) # [B, 2, N]\n",
    "#     valid = data['matches']['valid'].unsqueeze(0).to(device) # [B, N]\n",
    "\n",
    "    # [1,D,H,W]\n",
    "    out_a = model.forward(rgb_tensor_a)\n",
    "    des_img_a = out_a['descriptor_image']\n",
    "    \n",
    "    # sample reference descriptors\n",
    "    img_mask = torch.tensor(data['data_a']['mask']).to(device)\n",
    "    print(\"img_mask.shape\", img_mask.shape)\n",
    "    ref_descriptors_dict = sample_descriptors(des_img_a.squeeze(), img_mask, NUM_REF_DESCRIPTORS)\n",
    "    \n",
    "    ref_descriptors = ref_descriptors_dict['descriptors']\n",
    "    ref_descriptors_indices = ref_descriptors_dict['indices']\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"ref_descriptors.shape\", ref_descriptors.shape)\n",
    "        print(\"ref_descriptors_indices.shape\", ref_descriptors_indices.shape)\n",
    "\n",
    "    # make PrecomputeDescriptorNet\n",
    "    pdn = PrecomputedDescriptorNet(global_config)\n",
    "    pdn._ref_descriptors.data = ref_descriptors\n",
    "\n",
    "\n",
    "\n",
    "# draw reference image\n",
    "label_color = [0, 255, 0]\n",
    "rgb_a_wr = np.copy(rgb_a)\n",
    "vis_utils.draw_reticles(rgb_a_wr,\n",
    "                        ref_descriptors_indices[:, 0],\n",
    "                        ref_descriptors_indices[:, 1],\n",
    "                        label_color)\n",
    "\n",
    "figname = \"reference descriptors\"\n",
    "fig = plt.figure(figname, figsize=figsize)\n",
    "axes = fig.subplots(num_rows, num_cols, squeeze=False)\n",
    "axes[0,0].imshow(rgb_a_wr)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize reference descriptors in new episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "pdc_utils.reset_random_seed()\n",
    "\n",
    "sz = 2\n",
    "num_rows = 1\n",
    "num_cols = 2\n",
    "figsize = (6.4*sz*num_cols, 4.8*sz*num_rows)\n",
    "\n",
    "# this is a randomly selected episode change the idx to\n",
    "# get a different one\n",
    "episode_1 = multi_episode_dict[episode_name_list[3]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(100):\n",
    "        idx_cur = i*idx_step\n",
    "        \n",
    "        if idx_cur >= episode_1.length:\n",
    "            break\n",
    "        \n",
    "        data = dataset_tmp._getitem(episode_1,\n",
    "                                idx_cur,\n",
    "                                camera_name_a=camera_name_a, \n",
    "                                camera_name_b=camera_name_b)\n",
    "        \n",
    "        rgb = data['data_a']['rgb']\n",
    "        rgb_tensor = data['data_a']['rgb_tensor'].unsqueeze(0).to(device)\n",
    "        descriptor_net_out = model.forward(rgb_tensor)\n",
    "        des_img = descriptor_net_out['descriptor_image']\n",
    "        \n",
    "        # try doing this with precomputed descriptors\n",
    "        # instead of passing it forwards through the model to compute\n",
    "        # des_img, this should be a good sanity check\n",
    "        \n",
    "        \n",
    "        \n",
    "        des_img_precomputed = torch.Tensor(data['data_a']['descriptor']).to(device).unsqueeze(0)\n",
    "        print(\"des_img_precomputed.shape\", des_img_precomputed.shape)\n",
    "        print(\"des_img.shape\", des_img.shape)\n",
    "        \n",
    "        des_img_delta_norm = torch.norm(des_img - des_img_precomputed)\n",
    "        print(\"des_img_delta norm\", des_img_delta_norm)\n",
    "        \n",
    "#         print(\"des_img\", des_img[0,0,:,:])\n",
    "#         print(\"des_img_precomputed\", des_img_precomputed[0,0,:,:])\n",
    "        \n",
    "        \n",
    "        out = pdn.forward_descriptor_image(des_img)\n",
    "#         out = pdn.forward_descriptor_image(des_img_precomputed)\n",
    "        \n",
    "        \n",
    "        # [N, 2]\n",
    "        best_match_indices = out['best_match_dict']['indices'].squeeze()\n",
    "\n",
    "\n",
    "        # draw reference image\n",
    "        label_color = [0, 255, 0]\n",
    "        rgb_a_wr = np.copy(rgb_a)\n",
    "        vis_utils.draw_reticles(rgb_a_wr,\n",
    "                                ref_descriptors_indices[:, 0],\n",
    "                                ref_descriptors_indices[:, 1],\n",
    "                                label_color)\n",
    "        \n",
    "        # draw target image\n",
    "        label_color = [255,0,0]\n",
    "        rgb_wr = np.copy(rgb)\n",
    "        vis_utils.draw_reticles(rgb_wr,\n",
    "                               best_match_indices[:, 0],\n",
    "                               best_match_indices[:, 1],\n",
    "                               label_color)\n",
    "        \n",
    "        \n",
    "        figname = \"target idx: %d\" %(idx_cur)\n",
    "        fig = plt.figure(figname, figsize=figsize)\n",
    "        axes = fig.subplots(num_rows, num_cols, squeeze=False)\n",
    "        axes[0,0].imshow(rgb_a_wr)\n",
    "        axes[0,1].imshow(rgb_wr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 640\n",
    "H = 480\n",
    "image_diagonal_pixels = np.sqrt(H**2 + W**2)\n",
    "image_diagonal_pixels * 0.00625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*250/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
