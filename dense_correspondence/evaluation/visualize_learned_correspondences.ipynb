{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pdc\n",
    "from dense_correspondence_manipulation.utils.utils import set_cuda_visible_devices\n",
    "\n",
    "GPU_LIST = [0]\n",
    "set_cuda_visible_devices(GPU_LIST)\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "# key_dynam\n",
    "from key_dynam.dataset.episode_dataset import MultiEpisodeDataset\n",
    "from key_dynam.dynamics.utils import rand_int, count_trainable_parameters, Tee, AverageMeter, get_lr, to_np, set_seed\n",
    "from key_dynam.dataset.function_factory import ObservationFunctionFactory, ActionFunctionFactory\n",
    "from key_dynam.dataset.utils import load_drake_sim_episodes_from_config\n",
    "from key_dynam.utils.utils import save_yaml, get_image_diagonal_from_config\n",
    "from key_dynam.models.model_builder import build_visual_dynamics_model\n",
    "from key_dynam.dense_correspondence.descriptor_net import sample_descriptors\n",
    "from key_dynam.utils.utils import get_project_root, load_yaml, get_current_YYYY_MM_DD_hh_mm_ss_ms, get_data_root\n",
    "import dense_correspondence_manipulation.utils.visualization as vis_utils\n",
    "\n",
    "from key_dynam.utils.utils import get_project_root, load_yaml\n",
    "from key_dynam.dataset.drake_sim_episode_reader import DrakeSimEpisodeReader\n",
    "from key_dynam.utils import transform_utils\n",
    "from key_dynam.utils import drake_image_utils\n",
    "\n",
    "# pdc\n",
    "from dense_correspondence.dataset.dynamic_drake_sim_dataset import DynamicDrakeSimDataset\n",
    "from dense_correspondence.correspondence_tools.correspondence_finder import reproject_pixels\n",
    "from dense_correspondence.correspondence_tools import correspondence_plotter\n",
    "from dense_correspondence.correspondence_tools.correspondence_finder import compute_correspondence_data, pad_correspondence_data\n",
    "from dense_correspondence_manipulation.utils.utils import getDenseCorrespondenceSourceDir\n",
    "import dense_correspondence.loss_functions.utils as loss_utils\n",
    "import dense_correspondence_manipulation.utils.utils as pdc_utils\n",
    "from dense_correspondence.network import predict\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n')\n",
    "print(\"device_count\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_NAME = \"2019-12-05-15-58-48-462834_top_down_rotated_250\"\n",
    "LOAD_DESCRIPTOR_IMAGES = True\n",
    "\n",
    "\n",
    "dataset_root = os.path.join(get_project_root(), \"data/dev/experiments/05/data\", DATASET_NAME)\n",
    "\n",
    "\n",
    "\n",
    "multi_episode_dict = DrakeSimEpisodeReader.load_dataset(dataset_root)\n",
    "# make pdc dataset now\n",
    "\n",
    "# placeholder for now\n",
    "config_file = os.path.join(getDenseCorrespondenceSourceDir(), \n",
    "                           'config/dense_correspondence/global/drake_sim_dynamic.yaml')\n",
    "config = load_yaml(config_file)\n",
    "dataset = DynamicDrakeSimDataset(config, multi_episode_dict)\n",
    "\n",
    "W = 640\n",
    "H = 480\n",
    "diag = np.sqrt(W**2 + H**2)\n",
    "sigma = config['loss_function']['sigma_fraction'] * diag\n",
    "\n",
    "episode_name = list(multi_episode_dict.keys())[0]\n",
    "print(\"episode name:\", episode_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(get_data_root(), \n",
    "                          \"dev/experiments/05/trained_models/dense_descriptors/2019-12-27-22-29-23-359200_dataset_2019-12-05-15-58-48-462834_top_down_rotated_250/net_dy_epoch_55_iter_5000_model.pth\")\n",
    "\n",
    "model = torch.load(model_file)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a pair images, visualize ground truth an estimated correspondences\n",
    "\n",
    "sz = 3\n",
    "figsize = (6.4*sz, 4.8*sz)\n",
    "num_rows = 1\n",
    "num_cols = 2\n",
    "K = 4 # num matches to display\n",
    "\n",
    "\n",
    "episode_name = list(multi_episode_dict.keys())[0]\n",
    "episode = dataset.episodes[episode_name]\n",
    "camera_names = list(episode.camera_names)\n",
    "camera_name_a = camera_names[0]\n",
    "camera_name_b = camera_names[1]\n",
    "idx = 0\n",
    "data = dataset._getitem(episode, idx, camera_name_a, camera_name_b)\n",
    "\n",
    "\n",
    "## Visualize both rgb images with a few ground truth correspondences in green\n",
    "## Learned correspondences will be in red . . . \n",
    "\n",
    "figname = \"target idx: %d\" %(idx)\n",
    "fig = plt.figure(figname, figsize=figsize)\n",
    "axes = fig.subplots(num_rows, num_cols, squeeze=False)\n",
    "uv_a = data['matches']['uv_a'][:,:K] # [2, num_matches]\n",
    "uv_b = data['matches']['uv_b'][:,:K] # [2, num_matches]\n",
    "rgb_a = np.copy(data['data_a']['rgb'])\n",
    "rgb_b = np.copy(data['data_b']['rgb'])\n",
    "\n",
    "\n",
    "# draw reticles on image a\n",
    "label_color = [0, 255, 0]\n",
    "vis_utils.draw_reticles(rgb_a,\n",
    "                        uv_a[0, :],\n",
    "                        uv_a[1, :],\n",
    "                        label_color)\n",
    "\n",
    "ax = axes[0,0]\n",
    "ax.imshow(rgb_a)\n",
    "\n",
    "label_color = [0, 255, 0]\n",
    "vis_utils.draw_reticles(rgb_b,\n",
    "                        uv_b[0, :],\n",
    "                        uv_b[1, :],\n",
    "                        label_color)\n",
    "ax = axes[0,1]\n",
    "ax.imshow(rgb_b)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # now localize the correspondences\n",
    "    # push rgb_a, rgb_b through the network\n",
    "    rgb_tensor_a = data['data_a']['rgb_tensor'].unsqueeze(0).cuda()\n",
    "    out_a = model.forward(rgb_tensor_a)\n",
    "    des_img_a = out_a['descriptor_image']\n",
    "\n",
    "    rgb_tensor_b = data['data_b']['rgb_tensor'].unsqueeze(0).cuda()\n",
    "    out_b = model.forward(rgb_tensor_b)\n",
    "    des_img_b = out_b['descriptor_image']\n",
    "\n",
    "    # extract descriptors corresponding to uv_a in des_a\n",
    "    # [B, K, D]\n",
    "    des_a = pdc_utils.index_into_batch_image_tensor(des_img_a, uv_a.unsqueeze(0).cuda()).permute([0,2,1])\n",
    "    \n",
    "    # localize these in the other image\n",
    "    # find best match in des_img_b\n",
    "    \n",
    "    best_match_dict = predict.get_argmax_l2(des_a, des_img_b)\n",
    "    \n",
    "    # [2, K]\n",
    "    best_match_uv_b = best_match_dict['indices'].permute([0, 2, 1]).squeeze()\n",
    "\n",
    "    print(\"des_a.shape\", des_a.shape)\n",
    "    print(\"best_match_uv_b.shape\", best_match_uv_b.shape)\n",
    "\n",
    "    \n",
    "label_color = [255, 0, 0]\n",
    "vis_utils.draw_reticles(rgb_b,\n",
    "                        best_match_uv_b[0, :],\n",
    "                        best_match_uv_b[1, :],\n",
    "                        label_color)\n",
    "\n",
    "\n",
    "figname = \"learned correspondences\"\n",
    "fig = plt.figure(figname, figsize=figsize)\n",
    "axes = fig.subplots(num_rows, num_cols, squeeze=False)\n",
    "ax = axes[0,0]\n",
    "ax.imshow(rgb_a)\n",
    "\n",
    "ax = axes[0,1]\n",
    "ax.imshow(rgb_b)\n",
    "\n",
    "# compute heatmaps\n",
    "# this is the heatmap that an individual descriptor from rgb_a\n",
    "# will induce given descriptor image b (des_img_b)\n",
    "# [B,N,H,W]\n",
    "heatmap_pred = loss_utils.compute_heatmap_from_descriptors(des_a,\n",
    "                                                           des_img_b,\n",
    "                                                           sigma=sigma,\n",
    "                                                           type=config['loss_function']['heatmap_type'])\n",
    "\n",
    "\n",
    "# visualize heatmaps\n",
    "for k in range(K):\n",
    "    heatmap = heatmap_pred[0, k].detach().cpu().numpy() # [H, W]\n",
    "    heatmap_rgb = vis_utils.colormap_from_heatmap(heatmap)\n",
    "    figname = \"heatmap %k\"\n",
    "    fig = plt.figure(figname, figsize=(6.4, 4.8))\n",
    "    axes = fig.subplots(1, 1, squeeze=False)\n",
    "    ax = axes[0,0]\n",
    "    ax.imshow(heatmap_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
