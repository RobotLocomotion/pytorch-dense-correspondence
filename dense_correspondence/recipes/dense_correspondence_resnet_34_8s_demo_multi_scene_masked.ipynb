{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '../pytorch-segmentation-detection/vision/')\n",
    "sys.path.append('../pytorch-segmentation-detection/')\n",
    "\n",
    "# Use second GPU -pytorch-segmentation-detection- change if you want to use a first one\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import sys; sys.path.append('./dataset')\n",
    "import correspondence_plotter\n",
    "from labelfusion import LabelFusionDataset\n",
    "\n",
    "\n",
    "descriptor_dimensionality = 3\n",
    "nets = sorted(glob.glob(\"trained_models/5drillscenes_maskedmatches_\"+str(descriptor_dimensionality)+\"d/dense_resnet*.pth\"))\n",
    "print \"Networks:\"\n",
    "for net in nets:\n",
    "    print \"   - \", net\n",
    "\n",
    "lf = LabelFusionDataset()\n",
    "scene = \"2017-06-13-12\"\n",
    "\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = \"0000001000\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose(\n",
    "                [\n",
    "                     transforms.ToTensor(),\n",
    "                ])\n",
    "\n",
    "def forward_on_img(net, img):\n",
    "    img = valid_transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = Variable(img.cuda())\n",
    "    fcn = resnet_dilated.Resnet34_8s(num_classes=descriptor_dimensionality)\n",
    "    fcn.load_state_dict(torch.load(net))\n",
    "    fcn.cuda()\n",
    "    fcn.eval()\n",
    "    res = fcn(img)\n",
    "    res = res.squeeze(0)\n",
    "    res = res.permute(1,2,0)\n",
    "    res = res.data.cpu().numpy().squeeze()\n",
    "    return res\n",
    "\n",
    "res_a = forward_on_img(nets[0], img_a_rgb)\n",
    "res_b = forward_on_img(nets[0], img_b_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(res):\n",
    "    normed_res = res + -np.min(res)\n",
    "    normed_res = normed_res / np.max(normed_res)\n",
    "    return normed_res\n",
    "\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    axes = axes.flat[0:]\n",
    "    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth, normalize(res_a), normalize(res_b)]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does evolution of descriptors look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    for index, this_net in enumerate(nets):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        if index == 0:\n",
    "            axes[0].set_title(\"img a\")\n",
    "            axes[1].set_title(\"img b\")\n",
    "        axes[0].imshow(normalize(forward_on_img(this_net, img_a_rgb)))\n",
    "        axes[1].imshow(normalize(forward_on_img(this_net, img_b_rgb)))\n",
    "        axes[0].set_ylabel(this_net)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about overlaying these images?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_net = nets[-1]\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    axes[0].set_title(\"img a\")\n",
    "    axes[1].set_title(\"img b\")\n",
    "    axes[0].imshow(normalize(forward_on_img(this_net, img_a_rgb)))\n",
    "    axes[0].imshow(img_a_rgb, alpha=0.5)\n",
    "    axes[1].imshow(normalize(forward_on_img(this_net, img_b_rgb)))\n",
    "    axes[1].imshow(img_b_rgb, alpha=0.5)\n",
    "    axes[0].set_ylabel(this_net)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we plot the heatmap of correspondences between two images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import correspondence_finder as cf\n",
    "uv_a, uv_b = cf.batch_find_pixel_correspondences(img_a_depth, img_a_pose, \n",
    "                                img_b_depth, img_b_pose, \n",
    "                                num_attempts=100)\n",
    "\n",
    "    # uv_a = (390,390),(171,171)\n",
    "    # uv_b = (495,495),(322,322)\n",
    "\n",
    "# first element is width (640 max)\n",
    "# second element is heigh (480 max)\n",
    "# switch these here\n",
    "sample_from_a = (uv_a[1][0], uv_a[0][0])\n",
    "# now it's 480, 640\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "# shape is (480, 640, 3)\n",
    "descriptor_at_pixel = res_a[sample_from_a[0], sample_from_a[1]]\n",
    "print descriptor_at_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "norm_diffs = np.zeros_like(img_b_depth)*0.0\n",
    "for i, row in enumerate(norm_diffs):\n",
    "    for j, value in enumerate(row):\n",
    "        norm_diffs[i][j] = np.linalg.norm(res_b[i][j] - descriptor_at_pixel)**2\n",
    "\n",
    "print norm_diffs\n",
    "plt.imshow(norm_diffs, cmap=plt.cm.BuPu_r)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(norm_diffs, cmap=plt.cm.BuPu_r, vmax = 0.5)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "norm_diffs_numpy = np.sum(np.square(res_b - descriptor_at_pixel), axis=2)\n",
    "plt.imshow(norm_diffs, cmap=plt.cm.BuPu_r, vmax = 0.5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "fig, axes = plt.subplots(1, ncols=2)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "axes[0].set_title(\"img a\")\n",
    "axes[1].set_title(\"img b\")\n",
    "circ = Circle(sample_from_a[::-1], radius=10, facecolor='g', edgecolor='white', fill=True ,linewidth = 2.0, linestyle='solid')\n",
    "axes[0].add_patch(circ)\n",
    "axes[0].imshow(img_a_rgb, alpha=0.8)\n",
    "axes[1].imshow(norm_diffs, cmap=plt.cm.BuPu_r)\n",
    "axes[1].imshow(img_b_rgb, alpha=0.2)\n",
    "axes[0].set_ylabel(this_net)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold the heatmap, compute the best predicted match (purple circle), and \"ground truth\" best match (green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correspondence_heatmap(pixel_a, pixel_b, img_a_rgb, img_b_rgb, res_a, res_b):\n",
    "    descriptor_at_pixel = res_a[pixel_a[0], pixel_a[1]]\n",
    "    norm_diffs = np.zeros_like(img_b_depth)*0.0\n",
    "    for i, row in enumerate(norm_diffs):\n",
    "        for j, value in enumerate(row):\n",
    "            norm_diffs[i][j] = np.linalg.norm(res_b[i][j] - descriptor_at_pixel)**2\n",
    "    fig, axes = plt.subplots(1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    axes[0].set_title(\"img a\")\n",
    "    axes[1].set_title(\"img b\")\n",
    "    circ = Circle(pixel_a[::-1], radius=10, facecolor='g', edgecolor='white', fill=True ,linewidth = 2.0, linestyle='solid')\n",
    "    axes[0].add_patch(circ)\n",
    "    axes[0].imshow(img_a_rgb, alpha=0.8)\n",
    "    axes[1].imshow(norm_diffs, cmap=plt.cm.BuPu_r, vmax=2)\n",
    "    axes[1].imshow(img_b_rgb, alpha=0.2)\n",
    "    circ = Circle(pixel_b[::-1], radius=10, facecolor='g', edgecolor='white', fill=True ,linewidth = 2.0, linestyle='solid', alpha=0.8)\n",
    "    axes[1].add_patch(circ)\n",
    "    # plot best match\n",
    "    best_match = np.argmin(norm_diffs)\n",
    "    best_match = (best_match/640, best_match%640)\n",
    "    circ = Circle(best_match[::-1], radius=10, facecolor='purple', edgecolor='white', fill=True ,linewidth = 2.0, linestyle='solid', alpha=0.8)\n",
    "    axes[1].add_patch(circ)\n",
    "    axes[0].set_ylabel(this_net)\n",
    "    plt.show()\n",
    "    \n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "for i in range(len(uv_a[0])):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    sample_from_a = (uv_a[1][i], uv_a[0][i])\n",
    "    answer_from_b = (uv_b[1][i], uv_b[0][i])\n",
    "    make_correspondence_heatmap(sample_from_a, answer_from_b, img_a_rgb, img_b_rgb, res_a, res_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about correspondences from lots of different views, different scenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LabelFusionDataset()\n",
    "for i in range(10):\n",
    "    scene_directory = lf.get_random_scene_directory()\n",
    "    img_a_rgb, img_a_depth, img_a_pose = lf.get_random_rgbd_with_pose(scene_directory)\n",
    "    img_b_rgb, img_b_depth, img_b_pose = lf.get_different_rgbd_with_pose(scene_directory, img_a_pose)\n",
    "    uv_a, uv_b = cf.batch_find_pixel_correspondences(img_a_depth, img_a_pose, \n",
    "                                                    img_b_depth, img_b_pose, \n",
    "                                                    num_attempts=100)\n",
    "\n",
    "    res_a = forward_on_img(this_net, img_a_rgb)\n",
    "    res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "    sample_from_a = (uv_a[1][0], uv_a[0][0])\n",
    "    answer_from_b = (uv_b[1][0], uv_b[0][0])\n",
    "    make_correspondence_heatmap(sample_from_a, answer_from_b, img_a_rgb, img_b_rgb, res_a, res_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do the descriptors look like in different scenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-01\"\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "img_b_index = \"0000001000\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    axes = axes.flat[0:]\n",
    "    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth, normalize(res_a), normalize(res_b)]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_net = nets[-1]\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    axes[0].set_title(\"img a\")\n",
    "    axes[1].set_title(\"img b\")\n",
    "    axes[0].imshow(normalize(forward_on_img(this_net, img_a_rgb)))\n",
    "    axes[0].imshow(img_a_rgb, alpha=0.5)\n",
    "    axes[1].imshow(normalize(forward_on_img(this_net, img_b_rgb)))\n",
    "    axes[1].imshow(img_b_rgb, alpha=0.5)\n",
    "    axes[0].set_ylabel(this_net)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-15\"\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "img_b_index = \"0000001000\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    axes = axes.flat[0:]\n",
    "    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth, normalize(res_a), normalize(res_b)]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-16\"\n",
    "img_a_index = \"0000000200\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "img_b_index = \"0000001000\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    axes = axes.flat[0:]\n",
    "    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth, normalize(res_a), normalize(res_b)]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we try to find a match for the drill, in a different scene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-12\"\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "\n",
    "scene = \"2017-06-13-01\"\n",
    "img_b_index = \"0000000800\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "# I have to choose these samples by hand, since don't have ground truth for matching between scenes\n",
    "sample_from_a = (200, 350)\n",
    "answer_from_b = (200, 4500)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "make_correspondence_heatmap(sample_from_a, answer_from_b, img_a_rgb, img_b_rgb, res_a, res_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-12\"\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "\n",
    "scene = \"2017-06-13-15\"\n",
    "img_b_index = \"0000000001\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "# I have to choose these samples by hand, since don't have ground truth for matching between scenes\n",
    "sample_from_a = (140, 350)\n",
    "answer_from_b = (2500, 285)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "make_correspondence_heatmap(sample_from_a, answer_from_b, img_a_rgb, img_b_rgb, res_a, res_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"2017-06-13-12\"\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "\n",
    "scene = \"2017-06-13-16\"\n",
    "img_b_index = \"0000000201\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)\n",
    "\n",
    "# I have to choose these samples by hand, since don't have ground truth for matching between scenes\n",
    "sample_from_a = (145, 340)\n",
    "answer_from_b = (2500, 285)\n",
    "\n",
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "make_correspondence_heatmap(sample_from_a, answer_from_b, img_a_rgb, img_b_rgb, res_a, res_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
